# 1. DML 성능에 영향을 미치는 요소
- 인덱스
- 무결성 제약
- 조건절
- 서브쿼리
- Redo 로깅
- Undo 로깅
- Lock
- 커밋 
## ① 인덱스
- 테이블에 레코드를 입력하면 인덱스에도 입력해야 함 
- 인덱스는 정렬되어 있기 때문에, 수직적 탐색을 통해 입력할 블록을 찾아야 함
	- 테이블은 Freelist를 통해 입력할 수 있는 블록을 할당 받음
    - 인덱스에 입력하는 과정이 더 복잡하므로 DML 성능에 미치는 영향도 큼 
- 레코드를 삭제할 때도 인덱스 레코드를 모두 찾아서 삭제해야 함 
- 레코드를 수정할 때는 인덱스의 두 개의 오퍼레이션이 발생
	- 저장 위치가 달라지기 때문에 삭제 후 삽입이 진행

=> 인덱스가 DML 성능에 미치는 영향이 매우 큼
  - 인덱스 설계가 중요한 이유 
  - 핵심 트랜잭션 테이블에서 인덱스를 하나라도 줄이면 TPS는 향상됨

## ② 무결성 제약
- 데이터 무결성 규칙
	- 개체 무결성 Entity Integrity
    - 참조 무결성 Referential Integrity
    - 도메인 무결성 Domain Integrity
    - 사용자 정의 무결성 
 - PK,FK 제약은 실제 데이터를 조회해야 하므로 Check, Not Null 보다 성능에 큰 영향을 미침
 	- Check, Not Null은 정의한 제약 조건을 준수하는지만 확인하면 됨 
   
## ③ 조건절
- 인덱스 튜닝 원리 적용 가능

## ④ 서브쿼리
- 조인 튜닝 원리 적용 가능 

## ⑤ Redo 로깅
- 데이터파일과 컨트롤 파일의 변경 사항을 Redo 로그에 기록 
- 트랜잭션 데이터가 유실됐을 때, 트랜잭션을 제현함으로써 유실 이전 상태로 복구하는데 사용 
- DML 작업할 때마다 Redo 로그를 생성해야 하므로 Redo 로깅은 DML 성능에 영향을 미침 
	- `INSERT` 작업에 대해 Redo 로깅 생략 기능도 존재 

**Redo 로그 용도**
1. Database Recovery
- 물리적으로 디스크가 깨지는 등의 Media fail 발생 시 데이터를 복구하기 위해 사용 
- 온라인 Redo 로그를 백업한 Archived Redo 로그 이용 -> Media Recovery 

2. Cache Recovery (Instance Recovery 시 roll forward 단계)
- 캐시에 저장된 변경사항이 디스크에 기록되지 않은 상태로 인스턴스가 종료될 때 데이터를 복구
- 즉, 트랜잭션 데이터 유실에 대비하기 위해 Redo 로그를 남김 

3. Fast Commit 
- 데이터 변경을 디스크에 반영하는 작업은 랜덤 액세스 방식으로 매우 느림
- 로그는 Append 방식이기 때문에 상대적으로 빠름
- 트랜잭션에 의한 변경사항을 빠르게 로그 파일에 기록하고, 데이터 파일 동기화는 적절한 수단을 이용해 일괄 수행
	- 적절한 수단 : DBWR , Checkpoint
- 즉, 갱신 내용이 메모리상의 버퍼블록에만 기록되고 디스크에는 기록되지 않았지만 Redo 로그를 믿고 빠르게 커밋을 완료한다는 의미
	- Redo 로그 파일에 기록했다면 인스턴스 crash가 발생해도 언제든 복구 가능 

## ⑥ Undo 로깅 
- 트랜잭션을 롤백함으로써 현재를 과거 상태로 되돌리는데 사용 
- 이를 위해, 변경된 블록을 이전 상태로 되돌리는데 필요한 정보를 로깅
- Redo와 마찬가지로, DML을 작업할 때마다 Undo를 생성해야하므로 Undo 로깅은 DML 성능에 영향을 미침

**Undo 로그 용도**
1. Transaction Rollback
- 트랜잭션에 의한 변경사항을 최종 커밋하지 않고 롤백하고자 할 때 이용

2. Transaction Recovery (Instance Recovery 시 rollback 단계)
- Instance Crash 발생 후 redo를 이용해 roll forward 단계가 완료되면 최종 커밋되지 않은 변경사항까지 복구됨
- 시스템이 셧다운된 시점에 커밋되지 않은 트랜잭션들을 롤백하기 위해 사용 

3. Read Consistency
- 읽기 일관성을 위해 사용
	- 동시 트랜잭션이 많을수록 블록 I/O가 증가하면서 성능 저하로 이어짐 
 
**MVCC 모델**
- Multi-Version Concurrency Control 
- MVCC 모델을 사용하는 오라클은 데이터를 두 가지 모드로 읽음
	- Current 모드 : 디스크에서 캐시로 적재된 원본 블록을 현재 상태 그대로 읽는 방식 
    - Consistent 모드 : 쿼리가 시작된 이후라면 데이터가 변경되었더라도 시작된 시점으로 되돌려서 읽는 방식
    	- 원본 블록으로부터 복사본 블록을 만들고,
        - 복사본 블록에 Undo 데이터를 적용 
        - 원본 블록 하나에 여러 복사본이 캐시에 존재할 수 있음 
 
**SCN**
- System Commit Number
- 시스템에서 마지막 커밋이 발생한 시점정보를 관리하는 global 변수값
- 기본적으로 각 트랜잭션이 커밋할 때마다 1씩 증가하지만, 백그라운드 프로세서에 의해서도 조금씩 증가
- <u>블록 SCN</u> : 각 블록이 마지막으로 변경된 시점으로, 블록 헤어데 저장됨
- <u>쿼리 SCN</u> : 쿼리가 작업을 실행하기 전 읽은 SCN 값

**Consistent 모드**
- 쿼리 SCN과 블록 SCN을 비교함으로써 쿼리 수행 도중에 블록이 변경됐는지를 확인
- 데이터를 읽다가 블록 SCN이 쿼리 SCN보다 큰 블록을 만나면, 복사본 블록을 만들고 undo 데이터를 적용 
- undo 데이터가 다른 트랜잭션에 의해 재사용되어 쿼리 시작 시점으로 되돌리는 작업을 실패하면 오류 발생
	- Snapshot too old(ORA-01555) 에러 발생 
- SELECT 문은 항상 Consistent 모드로 데이터를 읽음 
- DML문은 Consistent 모드로 대상 레코드를 찾고, Current 모드로 추가/변경/삭제 
	- Consistent 모드로 DML 문이 시작된 시점에 존재했던 데이터 블록을 찾고
    - 다시 Current 모드로 원본 블록을 찾아서 갱신 
## ⑦ Lock
- Lock은 DML 성능에 매우 크고 직접적인 영향을 미침
- 성능과 데이터 품질이 트레이드 오프 관계
	- Lock을 자주 사용하거나 레벨을 높이면, DML 성능이 느려짐
    - Lock을 적게 사용하거나 레벨을 낮추면, 데이터 품질이 나빠짐
- 이를 위해 **동시성 제어** 가 필요
	- 동시에 실행되는 트랜잭션 수를 최대화하면서도 입력,수정,삭제,검색 시 데이터 무결성을 유지하기 위해 노력하는 것 
    
## ⑧ 커밋
- DML과 별개로 실행하지만, DML을 끝내려면 커밋까지 완료해야 하기 때문에 밀접한 관련이 존재 
- 특히, DML이 Lock에 의해 블로킹된 경우 커밋은 DML 성능과 직결 
	- DML을 완료할 수 있게 Lock을 푸는 방법이 커밋이기 때문 
### 커밋 내부 메커니즘

**1. DB 버퍼캐시**
- 서버 프로세스는 버퍼캐시를 통해 데이터를 읽고 씀
- DBWR 프로세스가 버퍼캐시에서 변경된 블록을 모아 주기적으로 데이터파일에 일괄 기록

**2. Redo 로그 버퍼**
- 버퍼캐시에 가한 변경 사항은 Redo 로그에도 기록됨
	- 버퍼캐시 데이터가 유실되더라도 Redo 로그로 복구 가능
- Redo 로그도 파일이므로, 디스크 I/O 발생 시 속도가 느린 문제 존재 
- Redo 로깅 성능 문제를 해결하기 위해 로그 버퍼 이용
	- Redo 로그 파일에 기록하기 전에 먼저 로그버퍼에 기록 
    - 로그버퍼에 기록한 내용은 LGWR 프로세스가 로그 파일에 일괄기록 

**3. 트랜잭션 데이터 저장 과정**
<div align="center">
  <img src="https://velog.velcdn.com/images/tothek/post/614f29e0-6d04-417c-8391-bcd733def9fa/image.jpg" width="550">
  </div>
  

  (1) DML 문을 실행하면 Redo 로그버퍼에 변경사항을 기록한다.
  (2) 버퍼블록에서 데이터를 변경한다. 버퍼캐시에서 블록을 찾지 못하면, 데이터 파일에서 읽는 작업부터 한다.
  (3) 커밋한다.
  (4) LGWR 프로세스가 Redo 로그버퍼 내용을 로그파일에 일괄 저장한다.
  (5) DBWR 프로세스가 변경된 버퍼블록들은 데이터파일에 일괄 저장한다.
  

- Log Force at Commit : 트랜잭션 영속성을 보장하기 위해, 적어도 커밋시점에는 Redo 로그버퍼 내용을 로그 파일에 기록 

**4. 커밋=저장버튼**
- 서버 프로세스가 지금까지 한 작업을 디스크에 기록하라는 명령어
- 저장을 완료할 때까지 서버 프로세스는 다음 작업진행 불가한 Sync 방식 
	- Redo 로그버퍼에 기록된 내용을 디스크에 기록하도록 LGWR 프로세스에 신호를 보낸 후 작업을 완료했다는 신호를 받아야 함 
    - LGWR 프로세스가 로그를 기록하는 작업은 디스크 I/O 작업으로 속도가 느림
- 따라서, 너무 자주 커밋하면 성능이 느려짐
- 오랫동안 커밋하지 않고 데이터를 갱신하면, Undo 공간이 부족해져 시스템 장애 상황 유발 가능 

# 2. 데이터베이스 Call
## SQL 실행
- Parse Call
	- SQL 파싱과 최적화를 수행하는 단계
    - SQL과 실행계획을 라이브러리 캐시에서 찾으면 최적화 단계 생략 가능
- Execute Call
	- SQL을 실행하는 단계
    - DML은 이 단계에서 모든 과정이 끝나고, SELECT 문은 Fetch 단계를 더 거침
- Fetch Call
	- 데이터를 읽어서 사용자에게 결과집합을 전송하는 과정
    - SELECT 문에서만 나타남
    - 전송할 데이터가 많으면 Fetch Call이 여러 번 발생 

## Call 발생 위치 
- User Call
	- 네트워크를 경유해 DBMS 외부로부터 인입되는 Call
    - 3-Tier 아키텍처에서의 User Call은 WAS 서버에서 발생하는 Call
- Recursive Call
	- DBMS 내부에서 발생하는 Call
    	- SQL 파싱과 최적화 과정에서 발생하는 데이터 딕셔너리 조회
        - PL/SQL로 작성한 사용자 정의 함수/프로시저/트리거에 내장된 SQL 실행 시 발생하는 Call
  
## 성능에 미치는 영향 
- 데이터베이스 Call이 많으면 성능이 느릴 수밖에 없음
- 특히, User Call은 네트워크를 경유하므로 성능에 많은 영향을 끼침 
- 절차적 프로그램을 **One SQL** 로 작성 시 성능 향상 가능
- One SQL로 구현하는데 유용한 구문
	- Insert Into Select
    - 수정가능 조인 뷰
    - Merge 문 

### Array Processing
- One SQL로 구현하는 일은 쉽지 않음
- **Array Processing** 기능을 활용하면 One SQL을 구현하지 않고도 Call 부하를 획기적으로 줄일 수 있음 
- 하나의 Call로 줄이지는 못해도, 10~100번 수준으로 줄일 수 있다면 One SQL에 준하는 성능효과 
```sql
SQL > declare
	2	cursor c is select * from source;
    3	type typ_source is table of c%rowtype;
    4	l_sorce typ_source;
    5
    6	l_array_size number default 10000;
    7
    8	procedure insert_target( p_source in typ_source) is 
    9	begin
    10	 forall i in p_source.first..p_source.last
    11	 	insert into target values p_source(i);
    12	 end insert_target;
    13
    14	 begin
    15		open c;
    16		loop
    17		 fetch c bulk collect into l_source limit l_array_size;
    18
    19		 insert_target(l_source);
    20
    21		 exit when c%notfound;
    22	    end loop;
    23
    24	    close c;
    25
    26      commit;
    27    end;
    28    /
```

# 3. 인덱스 및 제약 해제
> 동시 트랜잭션 없이 대량 데이터를 적재하는 배치 프로그램에서는 인덱스 및 제약을 해제하여 성능 개선 가능

1. PK 제약과 인덱스 해제
- PK 제약에 Unique 인덱스를 사용한 경우
	- PK 인덱스는 drop , X1 인덱스는 unusable 변경
    - PK 인덱스는 Unusable 상태에서 데이터를 입력할 수 없기 때문 
```sql
SQL > alter table target modify constraint target_pk disable drop index;
SQL > alter index target_x1 unsuable;
SQL > alter session set skip_unusable_indexes = true;
---- 작업
SQL > alter table target modify constraint target_pk enable NOVALIDATE;
```

- PK 제약에 Non-Unique 인덱스를 사용한 경우
	- PK 인덱스가 Unusable 상태에서 데이터를 입력하고 싶다면 Non-Unqiue 인덱스 사용 
```sql
SQL > alter table target drop primary key drop index;
SQL > create index target_pk on target(no,empno);
SQL > alter table target add
	2 constraint target_pk primary key (no,empno)
    3 using index target_pk;
    
SQL > alter table target modify constraint target_pk disable keep index;
SQL > alter index target_pk unusable;
SQL > alter index target_x1 unusable;
--- 작업
SQL > alter index target_x1 rebuild;
SQL > alter index target_pk rebuild;
SQL > alter table target modify constraint target_pk enable novalidate;
````

# 4. 수정 가능 조인 뷰 
- 전통적인 UPDATE 문을 사용하면 비효율을 완전히 해소할 수 없음
	- 같은 데이터를 여러번 조회하거나,
    - UPDATE 발생량이 너무 많아 비효율적이거나,
    - 모든 레코드에 Lock이 걸리거나,
    - 같은 값으로 갱신되는 비중이 높아 Redo 로그 발생량만 증가하거나
- **수정 가능 조인 뷰**를 활용하면 참조 테이블과 두 번 조인하는 비효율을 없앨 수 있음 
	- 조인 뷰 : FROM 절에 두 개 이상 테이블을 가진 뷰
    - 수정가능 조인 뷰 : 입력,수정,삭제가 허용되는 조인 뷰
    	- 단, 1쪽 집합과 조인하는 M쪽 집합에만 허용
        - 1쪽 집합에 PK 제약을 설정하거나 Unique 인덱스를 생성해야 함 
```sql
update
( select /*+ ordered use_hash(c) no_merge(t) */
		 c.최종거래일시, c.최근거래횟수, c.최근거래금액
         ,t.거래일시, t.거래횟수, t.거래금액
  from (select 고객번호
  				,max(거래일시) 거래일시, count(*) 거래횟수, sum(거래금액) 거래금액
        from 거래
        where 거래일시 >= trunc(add_months(sysdate,-1))
        group by 고객번호) t
        , 고객 c
  where c.고객번호 = t.고객번호
)
set 최종거래일시 = 거래일시
 ,  최근거래횟수 = 거래횟수
 ,  최근거래금액 = 거래금액
```
	- 12c 이상 버전에서만 정상 실행 가능
    - 10g 이하 버전에서는 UPDATE 옆에 `bypass_ujvc` 힌트 필요
    - 11g 에서는 실행 불가능 
    
## 키 보존 테이블
> 조인된 결과집합을 통해서도 중복 값 없이 Unique 하게 식별이 가능한 테이블 

- unique한 1쪽 집합과 조인되는 테이블이어야 조인된 결과집합을 통한 식별이 가능
- 즉, 뷰에 rowid를 제공하는 테이블
- 위의 쿼리 예시에서 아래 처럼 PK 제약을 설정하면 EMP 테이블이 키-보존 테이블이 됨
	- unique 인덱스 생성도 가능
```sql
SQL > alter table dept add constraint dept_pk primary key(deptno);
SQL > update EMP_DEPT_VIEW set comm = nvl(comm,0) + (sal * 0.1) where sal <= 1500;
```

### ORA-01779 오류
- 10g 에서는 bypass_ujvc 힌트를 이용해 제약을 회피
	- Updatable Join View를 생략하라는 의미
- 11g 부터 해당 힌트를 사용할 수 없으므로, MERGE 문으로 바꿔줘야 함 
- 12c 부터는 기능이 개선되어 힌트를 사용하지 않아도 실행 가능
	- Group by한 집합과 조인한 테이블은 키가 보존된다고 인식
    - 따라서, unqiue 인덱스가 없어도 group by 처리를 통해 에러 회피 가능 
```sql
update(
	select o.주문금액, o.할인금액
    from 주문_t o
    	,(select 고객번호 from 고객_t where 고객등급='A' group by 고객번호) c
    where o.고객번호 = c.고객번호
    and o.주문금액 >= 1000000)
set 할인금액 = 주문금액 * 0.2 , 주문금액 = 주문금액 * 0.8
```
- 배치 프로그램이나 데이터 이행 프로그램에서 사용하는 중간 임시 테이블에 유용한 패턴
	- 일일이 PK 제약이나 인덱스를 생성하지 않기 때문 

# 5. MERGE문 활용
- DW에서 가장 흔히 발생하는 오퍼레이션
: 기간계 시스템에서 가져온 신규 트랜잭션 데이터를 반영함으로써 두 시스템 간 데이터를 동기화하는 작업

- `ex` 고객 테이블에 발생한 변경분 데이터를 DW에 반영하는 프로세스
1. 전일 발생한 변경 데이터를 기간계 시스템으로부터 추출
2. CUSTOMER_DELTA 테이블을 DW 시스템으로 전송
3. DW 시스템으로 적재 

## MERGE문
> Source 테이블 기준으로 Target 테이블과 Left Outer 방식으로 조인해서 
조인에 성공하면 UPDATE , 실패하면 INSERT

- MERGE문을 UPSERT(=UPDATE + INSERT)라고도 부름 
- UPDATE와 INSERT의 선택적 처리도 가능
	- 해당 확장 기능을 통해 수정가능 조인 뷰 기능을 대체할 수 있음 
- ON 절에 기술한 조인문 외에 추가로 조건절 기술도 가능
- 이미 저장된 데이터를 조건에 따라 지우는 기능도 제공 
	- 이 때, 조인에 실패한 데이터는 UPDATE할 수도 없고 DELETE할 수도 없음 
 

